{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Multi-Channel Vision Transformer (TMC-ViT)\n",
        "\n",
        "**Author**: Ricardo V. Godoy <br>\n",
        "**Description**: This project implements a Transformer-based model called Temporal Multi-Channel Vision Transformer (TMC-ViT). The TMC-ViT was developed to adapt the Vision Transformer model proposed by [Dosovitskiy et al.](http://arxiv.org/abs/2010.11929) for processing multi-channel temporal signals as input. In this example, we will predict 18 gestures from the [Ninapro DB05\n",
        "Database](https://doi.org/10.1371/journal.pone.0186132). <br>\n",
        "**Requirements**: This model is developed in Keras 2.8.0 and Python 3.7.x."
      ],
      "metadata": {
        "id": "cnwMDqTCFWae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries\n"
      ],
      "metadata": {
        "id": "CITzpWiMGp0a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "semeTxLNFJJy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dropout, Conv2D, MaxPooling2D, BatchNormalization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data\n",
        "The input data must already be divided into training and test sets, with 200 ms samples. Use one separate repetition for testing. More information on the data preprocessing can be found in [Electromyography-Based, Robust Hand Motion Classification Employing Temporal Multi-Channel Vision Transformers](https://ieeexplore.ieee.org/document/9834070).\n"
      ],
      "metadata": {
        "id": "5K2na9pj0KJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the inputs\n",
        "X_test = np.load('./X_test.npy')\n",
        "X_train = np.load('./X_train.npy')\n",
        "y_test = np.load('./Y_test.npy')\n",
        "y_train = np.load('./Y_train.npy')\n",
        "\n",
        "# Prepare the data\n",
        "X_train = X_train.reshape(-1, 16, 40, 1)\n",
        "X_test = X_test.reshape(-1, 16, 40, 1)"
      ],
      "metadata": {
        "id": "4FSM1qrH0GAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the model parameters"
      ],
      "metadata": {
        "id": "4mDuNdJHz50a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 18 # Number of output gestures (17 gestures + rest gesture)\n",
        "input_shape = ([X_train.shape[1], X_train.shape[2], 1]) # Input shape\n",
        "image_size1 = 16 # Number of emg channels\n",
        "image_size2= 20 # Number of time steps. Will be resized to this value\n",
        "patch_size = 4 # Size of each patch. In this case, 4x4\n",
        "num_patches = (image_size1 // patch_size) * (image_size2 // patch_size) # Number of patches\n",
        "projection_dim = 64 # Output dimension of all sub-layers in the enconder, as well as the embedding layers\n",
        "num_heads = 4 # Number of Multi-Head Attention\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8 # Number of transformer blocks\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
      ],
      "metadata": {
        "id": "XehKJx_CzrKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the MLP used at the end of the TMC-ViT model"
      ],
      "metadata": {
        "id": "MyZlTjo8Gtec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "_hUHn-FyGzvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the patch class, which extracts patches from the input signal"
      ],
      "metadata": {
        "id": "2r-SxeGCG5WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ],
      "metadata": {
        "id": "WFcCUbGDG11Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the patch encoder\n",
        "In this step, the patches are linearly projected and the postition embedding is added."
      ],
      "metadata": {
        "id": "248D-xHcHWfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "rMH2Ks_xHLg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the TMC-ViT classifier"
      ],
      "metadata": {
        "id": "_y4gL3iMHyFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tmc_vit_classifier(input_shape, token_emb, patch_size, num_patches,\n",
        "        projection_dim, transformer_layers, num_heads, transformer_units,\n",
        "        mlp_head_units, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Token embedding.\n",
        "    tokenemb = token_emb(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(tokenemb)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, \n",
        "            dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ],
      "metadata": {
        "id": "U7DioCQRHrAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create token embedding\n",
        "The TMC-ViT employs convoltional blocks composed of convolutional, batch normalization, max-pooling, and dropout layers to both reduce the input dimension and extract the embeddings."
      ],
      "metadata": {
        "id": "d92JGT4uc5Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_emb = keras.Sequential(\n",
        "    [\n",
        "        Conv2D(16, (8, 8), activation=\"relu\", padding=\"same\", \n",
        "                input_shape=[X_train.shape[1], X_train.shape[2], 1]),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((1, 2)),\n",
        "        Dropout(0.3),\n",
        "        Conv2D(32, (4, 4), activation=\"relu\", padding=\"same\"),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Conv2D(projection_dim, (2, 2), activation=\"relu\", padding=\"same\"),\n",
        "        BatchNormalization(),\n",
        "    ],\n",
        "    name=\"token_emb\",\n",
        ")"
      ],
      "metadata": {
        "id": "1l1lQy8ccNNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the classifier"
      ],
      "metadata": {
        "id": "QmA0xAQB0Yty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_tmc_vit_classifier(input_shape, token_emb, patch_size, \n",
        "        num_patches, projection_dim, transformer_layers, num_heads, \n",
        "        transformer_units, mlp_head_units, num_classes)"
      ],
      "metadata": {
        "id": "iD5ejDSua6Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile the model"
      ],
      "metadata": {
        "id": "A6Hh_SWJ0Rqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', \n",
        "        metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "zTxZB0QybCYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define callback \n",
        "In this case, we will be using early stop"
      ],
      "metadata": {
        "id": "g33DsWEGbFgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
        "        min_delta=0, patience=70, mode='max', restore_best_weights=True)"
      ],
      "metadata": {
        "id": "uoSkoTNibPsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "kgZbSeWubWTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size=128, epochs=500, \n",
        "        verbose = 1, validation_data=(X_test, y_test),callbacks=[callback])"
      ],
      "metadata": {
        "id": "PYkhKbTWbTAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model"
      ],
      "metadata": {
        "id": "22Ekndn8LBI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "ujGgqyiHLDZW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}