{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, gradcheck\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import mne\n",
    "from mne.io import concatenate_raws, read_raw_fif\n",
    "import mne.viz\n",
    "\n",
    "import math\n",
    "\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-0.698</th>\n",
       "      <th>-0.762</th>\n",
       "      <th>-0.944</th>\n",
       "      <th>-1.122</th>\n",
       "      <th>-1.108</th>\n",
       "      <th>-0.723</th>\n",
       "      <th>0.07</th>\n",
       "      <th>1.125</th>\n",
       "      <th>2.155</th>\n",
       "      <th>2.898</th>\n",
       "      <th>...</th>\n",
       "      <th>20.833</th>\n",
       "      <th>20.093</th>\n",
       "      <th>19.654</th>\n",
       "      <th>19.536</th>\n",
       "      <th>19.697</th>\n",
       "      <th>20.011</th>\n",
       "      <th>20.335</th>\n",
       "      <th>20.588</th>\n",
       "      <th>20.827</th>\n",
       "      <th>21.199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-3.356</td>\n",
       "      <td>-3.230</td>\n",
       "      <td>-2.968</td>\n",
       "      <td>-2.780</td>\n",
       "      <td>-2.813</td>\n",
       "      <td>-3.067</td>\n",
       "      <td>-3.422</td>\n",
       "      <td>-3.729</td>\n",
       "      <td>-3.881</td>\n",
       "      <td>-3.842</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.171</td>\n",
       "      <td>-5.669</td>\n",
       "      <td>-6.558</td>\n",
       "      <td>-7.532</td>\n",
       "      <td>-8.214</td>\n",
       "      <td>-8.319</td>\n",
       "      <td>-7.755</td>\n",
       "      <td>-6.610</td>\n",
       "      <td>-5.066</td>\n",
       "      <td>-3.331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.936</td>\n",
       "      <td>-1.833</td>\n",
       "      <td>-2.833</td>\n",
       "      <td>-3.619</td>\n",
       "      <td>-3.919</td>\n",
       "      <td>-3.637</td>\n",
       "      <td>-2.916</td>\n",
       "      <td>-2.085</td>\n",
       "      <td>-1.501</td>\n",
       "      <td>-1.373</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.416</td>\n",
       "      <td>-2.966</td>\n",
       "      <td>-2.921</td>\n",
       "      <td>-3.143</td>\n",
       "      <td>-3.330</td>\n",
       "      <td>-3.237</td>\n",
       "      <td>-2.777</td>\n",
       "      <td>-1.950</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.232</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.805</td>\n",
       "      <td>-1.356</td>\n",
       "      <td>-1.463</td>\n",
       "      <td>-1.185</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.963</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.272</td>\n",
       "      <td>-2.243</td>\n",
       "      <td>-3.477</td>\n",
       "      <td>-4.764</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>-6.248</td>\n",
       "      <td>-5.933</td>\n",
       "      <td>-4.799</td>\n",
       "      <td>-2.954</td>\n",
       "      <td>-0.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.585</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.846</td>\n",
       "      <td>-7.245</td>\n",
       "      <td>-6.739</td>\n",
       "      <td>-6.557</td>\n",
       "      <td>-6.794</td>\n",
       "      <td>-7.355</td>\n",
       "      <td>-8.004</td>\n",
       "      <td>-8.513</td>\n",
       "      <td>-8.786</td>\n",
       "      <td>-8.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4.899</td>\n",
       "      <td>5.001</td>\n",
       "      <td>4.836</td>\n",
       "      <td>4.464</td>\n",
       "      <td>3.933</td>\n",
       "      <td>3.295</td>\n",
       "      <td>2.640</td>\n",
       "      <td>2.102</td>\n",
       "      <td>1.814</td>\n",
       "      <td>1.850</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.848</td>\n",
       "      <td>-2.510</td>\n",
       "      <td>-2.391</td>\n",
       "      <td>-2.514</td>\n",
       "      <td>-2.775</td>\n",
       "      <td>-2.982</td>\n",
       "      <td>-2.920</td>\n",
       "      <td>-2.431</td>\n",
       "      <td>-1.453</td>\n",
       "      <td>-0.030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    -0.698  -0.762  -0.944  -1.122  -1.108  -0.723   0.07  1.125  2.155  \\\n",
       "58  -3.356  -3.230  -2.968  -2.780  -2.813  -3.067 -3.422 -3.729 -3.881   \n",
       "59  -0.936  -1.833  -2.833  -3.619  -3.919  -3.637 -2.916 -2.085 -1.501   \n",
       "60   1.232   0.121  -0.805  -1.356  -1.463  -1.185 -0.671 -0.093  0.447   \n",
       "61   0.585   0.034  -0.144   0.065   0.452   0.721  0.667  0.278 -0.293   \n",
       "62   4.899   5.001   4.836   4.464   3.933   3.295  2.640  2.102  1.814   \n",
       "\n",
       "    2.898  ...  20.833  20.093  19.654  19.536  19.697  20.011  20.335  \\\n",
       "58 -3.842  ...  -5.171  -5.669  -6.558  -7.532  -8.214  -8.319  -7.755   \n",
       "59 -1.373  ...  -3.416  -2.966  -2.921  -3.143  -3.330  -3.237  -2.777   \n",
       "60  0.963  ...  -1.272  -2.243  -3.477  -4.764  -5.790  -6.248  -5.933   \n",
       "61 -0.810  ...  -7.846  -7.245  -6.739  -6.557  -6.794  -7.355  -8.004   \n",
       "62  1.850  ...  -2.848  -2.510  -2.391  -2.514  -2.775  -2.982  -2.920   \n",
       "\n",
       "    20.588  20.827  21.199  \n",
       "58  -6.610  -5.066  -3.331  \n",
       "59  -1.950  -0.725   0.973  \n",
       "60  -4.799  -2.954  -0.630  \n",
       "61  -8.513  -8.786  -8.876  \n",
       "62  -2.431  -1.453  -0.030  \n",
       "\n",
       "[5 rows x 384 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at some of the data\n",
    "data_file = 'study1_eeg/study1_EEG_P-01_FN_Trial-001.csv'\n",
    "\n",
    "data_P_09 = pd.read_csv(data_file)\n",
    "data_P_09.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<EpochsFIF  |   380 events (all good), 0 - 1.49609 sec, baseline off, ~71.4 MB, data loaded,\n",
      " 'FN': 109\n",
      " 'FP': 100\n",
      " 'FU': 83\n",
      " 'NN': 32\n",
      " 'NP': 35\n",
      " 'NU': 21>\n"
     ]
    }
   ],
   "source": [
    "# take some data that was already formatted, from this link: https://neuro.inf.unibe.ch/AlgorithmsNeuroscience/Tutorial_files/DatasetConstruction.html\n",
    "data_file = 'study1_eeg/epochdata/P-09'\n",
    "\n",
    "# Read the EEG epochs:\n",
    "epochs = mne.read_epochs(data_file + '.fif', verbose='error')\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<EpochsFIF  |   192 events (all good), 0 - 1.49609 sec, baseline off, ~36.2 MB, data loaded,\n",
      " 'FN': 109\n",
      " 'FU': 83>\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "epochs_UN = epochs['FU', 'FN'] # Unpleasant vs. Neutral\n",
    "epochs_UP = epochs['FU', 'FP'] # Unpleasant vs. Pleasant\n",
    "epochs_NP = epochs['FN', 'FP'] # Neutral vs. Pleasant\n",
    "\n",
    "# Dataset with unpleasant and neutral events\n",
    "print(epochs_UN)\n",
    "data_UN = epochs_UN.get_data() #we will classify between unpleasant and neutral\n",
    "labels_UN = epochs_UN.events[:,-1]\n",
    "print(len(labels_UN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_UN, test_data_UN, labels_train_UN, labels_test_UN = train_test_split(data_UN, labels_UN, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134,) (58,)\n"
     ]
    }
   ],
   "source": [
    "print(labels_train_UN.shape, labels_test_UN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([134, 1, 64, 384]) torch.Size([134, 1]) torch.Size([58, 1, 64, 384])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 27\n",
    "\n",
    "eeg_data_scaler = StandardScaler()\n",
    "\n",
    "X_train = eeg_data_scaler.fit_transform(train_data_UN.reshape(-1, train_data_UN.shape[-1])).reshape(train_data_UN.shape)\n",
    "X_test = eeg_data_scaler.fit_transform(test_data_UN.reshape(-1, test_data_UN.shape[-1])).reshape(test_data_UN.shape)\n",
    "\n",
    "X_train1 = np.zeros((134, 384, 64))\n",
    "X_test1 = np.zeros((58, 384, 64))\n",
    "\n",
    "for x in range(X_train.shape[0]):\n",
    "    X_train1[x] = X_train[x].T\n",
    "for x in range(X_test.shape[0]):\n",
    "    X_test1[x] = X_test[x].T\n",
    "\n",
    "labels_train_UN = np.array([1 if x > 0 else 0 for x in labels_train_UN])\n",
    "labels_test_UN = np.array([1 if x > 0 else 0 for x in labels_test_UN])\n",
    "\n",
    "labels_train_UN = labels_train_UN.reshape((134, 1))\n",
    "labels_train_UN = labels_train_UN.astype(np.float32)\n",
    "X_actual = torch.from_numpy(labels_train_UN)\n",
    "\n",
    "labels_test_UN = labels_test_UN.reshape((58, 1))\n",
    "labels_test_UN = labels_test_UN.astype(np.float32)\n",
    "X_test_actual = torch.from_numpy(labels_test_UN)\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_train = X_train.unsqueeze(1)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "X_test = X_test.unsqueeze(1)\n",
    "\n",
    "X_list = [0] * (math.ceil(X_train1.shape[0] / BATCH_SIZE))\n",
    "for i in range(len(X_list)):\n",
    "    a, b = BATCH_SIZE * i, BATCH_SIZE * (i + 1)\n",
    "    if i != len(X_list) - 1:\n",
    "        X_list[i] = (X_train[a:b, :, : ], X_actual[a:b, :])\n",
    "    else:\n",
    "        X_list[i] = (X_train[a:, :, : ], X_actual[a:, :])\n",
    "\n",
    "\n",
    "print(X_train.shape, X_actual.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_neurons = 4\n",
    "conv2_neurons = 8\n",
    "conv3_neurons = 4\n",
    "flat1_in = 168\n",
    "flat1_out = 64\n",
    "flat2_out = 256\n",
    "flat3_out = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNPoor = nn.Sequential(\n",
    "    nn.Conv2d(1, conv1_neurons, (1, 128), padding=1),\n",
    "    nn.ELU(),\n",
    "    nn.BatchNorm2d(conv1_neurons, False),\n",
    "    \n",
    "    nn.Conv2d(conv1_neurons, conv2_neurons, (64, 1), padding=1),\n",
    "    nn.ELU(),\n",
    "    nn.BatchNorm2d(conv2_neurons, False),\n",
    "    nn.AvgPool2d((1, 4)),\n",
    "    nn.Dropout(),\n",
    "    \n",
    "    nn.Conv2d(conv2_neurons, conv3_neurons, (1, 16), padding=1),\n",
    "    nn.ELU(),\n",
    "    nn.BatchNorm2d(conv3_neurons, False),\n",
    "    nn.AvgPool2d((1, 8)),\n",
    "    nn.Dropout(),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "\n",
    "    nn.Linear(flat1_in, flat1_out),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(flat1_out, 1),\n",
    "    nn.Sigmoid(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(CNNPoor.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        content, labels = data\n",
    "        pred = model(content)\n",
    "        pred = pred.numpy()\n",
    "    return accuracy_score(labels, np.round(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Total loss =  3.3932474851608276\n",
      "Train accuracy =  0.6417910447761194\n",
      "Test accuracy =  0.5689655172413793\n",
      "Epoch:  1\n",
      "Total loss =  3.21981418132782\n",
      "Train accuracy =  0.6194029850746269\n",
      "Test accuracy =  0.5517241379310345\n",
      "Epoch:  2\n",
      "Total loss =  2.981822669506073\n",
      "Train accuracy =  0.7089552238805971\n",
      "Test accuracy =  0.4827586206896552\n",
      "Epoch:  3\n",
      "Total loss =  2.935464024543762\n",
      "Train accuracy =  0.7238805970149254\n",
      "Test accuracy =  0.5517241379310345\n",
      "Epoch:  4\n",
      "Total loss =  2.691382795572281\n",
      "Train accuracy =  0.7388059701492538\n",
      "Test accuracy =  0.5689655172413793\n",
      "Epoch:  5\n",
      "Total loss =  2.5164186656475067\n",
      "Train accuracy =  0.7835820895522388\n",
      "Test accuracy =  0.5517241379310345\n",
      "Epoch:  6\n",
      "Total loss =  2.2277219593524933\n",
      "Train accuracy =  0.7835820895522388\n",
      "Test accuracy =  0.603448275862069\n",
      "Epoch:  7\n",
      "Total loss =  2.0274892449378967\n",
      "Train accuracy =  0.8134328358208955\n",
      "Test accuracy =  0.5862068965517241\n",
      "Epoch:  8\n",
      "Total loss =  2.0276423394680023\n",
      "Train accuracy =  0.8656716417910447\n",
      "Test accuracy =  0.5689655172413793\n",
      "Epoch:  9\n",
      "Total loss =  1.9607233107089996\n",
      "Train accuracy =  0.8432835820895522\n",
      "Test accuracy =  0.603448275862069\n",
      "Epoch:  10\n",
      "Total loss =  1.799261897802353\n",
      "Train accuracy =  0.8955223880597015\n",
      "Test accuracy =  0.603448275862069\n",
      "Epoch:  11\n",
      "Total loss =  1.484617531299591\n",
      "Train accuracy =  0.8955223880597015\n",
      "Test accuracy =  0.5689655172413793\n",
      "Epoch:  12\n",
      "Total loss =  1.2543281763792038\n",
      "Train accuracy =  0.8432835820895522\n",
      "Test accuracy =  0.6206896551724138\n",
      "Epoch:  13\n",
      "Total loss =  1.345233365893364\n",
      "Train accuracy =  0.9029850746268657\n",
      "Test accuracy =  0.6896551724137931\n",
      "Epoch:  14\n",
      "Total loss =  1.13565132021904\n",
      "Train accuracy =  0.9104477611940298\n",
      "Test accuracy =  0.6724137931034483\n",
      "Epoch:  15\n",
      "Total loss =  0.9734356254339218\n",
      "Train accuracy =  0.9402985074626866\n",
      "Test accuracy =  0.6724137931034483\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    print(\"Epoch: \", i)\n",
    "    tot_loss = 0.0\n",
    "\n",
    "    for j in range(math.ceil(X_train1.shape[0] / BATCH_SIZE)):\n",
    "        data, labels = X_list[j]\n",
    "        data, labels = Variable(data.float()), Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        classification = CNNPoor(data)\n",
    "        loss = loss_function(classification, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tot_loss += loss.item()\n",
    "    print(\"Total loss = \", tot_loss)\n",
    "    print(\"Train accuracy = \", evaluate(CNNPoor, (X_train.float(), X_actual)))\n",
    "    print(\"Test accuracy = \", evaluate(CNNPoor, (X_test.float(), X_test_actual)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
